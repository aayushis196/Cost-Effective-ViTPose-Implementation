# -*- coding: utf-8 -*-
"""VIT_Modules.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_YxCTYzoG06F8gI2yTXM6h4Q2P4jRGCo
"""

import math
import numpy as np
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import os
import cv2
from torch import nn, optim
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torch import Tensor
from PIL import Image
from pycocotools.coco import COCO
import torchvision.transforms as transforms
from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange, Reduce
from torchsummary import summary

# a=torch.rand(1, 196, 768)
# b= rearrange(a,  "b n (h d qkv) -> (qkv) b h n d", h=2, qkv=3)
# c=a.reshape(3, )permute(0, 2, 3, 1).reshape(1, -1, 3)
# print(b.size(), c.size())
# print(b==c)

''' Load Pretrained weights for training'''
pre_trained_weights_path = "/home/biped-lab/504_project/pretrain/mae_pretrain_vit_base.pth"  
pre_trained_weights = torch.load(pre_trained_weights_path)                                       
weights = pre_trained_weights["model"]

'''Select Device'''
device = "cpu" #Change to cuda when training

'''1. Embed the image into smaller Patches 
   2. Concatenate Positional embedding to each patch
   3. Add Pretrained Weights
'''

class PatchEmbedding(nn.Module):
    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: tuple = (224,224)):

        self.patch_size = patch_size
        self.img_size = img_size
        self.emb_size = emb_size
        super().__init__()
        '''We could have used either a conv layer or just reduced to patches, however ViTPose paper states conv gives better accuracy'''
        self.conv = nn.conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)
         
        '''No need of class token here, only positional embedding is needed, as opposed to ViT paper, which was for for specifically classification'''
        self.positions = self.reshape_position_embedding()
        
        self.conv.weight = nn.Parameter(weights["patch_embed.proj.weight"])
        self.conv.bias = nn.Parameter(weights["patch_embed.proj.bias"])

        
                
    def forward(self, x: Tensor) -> Tensor:
        x = self.conv(x)                     #This returns a tensor of shape b x out_channels x (input_dim/patch_size)*(input_dim/patch_size), out_channels=3 x patch_size**2
        batch_size, channels, h, w = x.shape
        x=x.permute(0, 2, 3, 1).reshape(batch_size, -1, channels)        # Flatten along h x w
        x  =  x + self.positions[:,1:] + self.positions[:,:1]
        return x
    
    '''This function is required for reshaping the weights we have for 224x224 resolution images
       to our image resolution using bicubic interpolation, and then adding those weights'''
    def reshape_position_embedding(self):

        pos_embed_temp = nn.Parameter(weights["pos_embed"])
        num_patches = (self.img_size[0]//self.patch_size) * (self.img_size[1]//self.patch_size)

        '''#he shape of pos embedding is h x w +1 (As original implementation has a class token)'''
        original_size = int((pos_embed_temp.shape[-2] - 1)**0.5)         
        patches_along_height = (self.img_size[0]//self.patch_size)
        patches_along_width =  (self.img_size[1]//self.patch_size)
        cls_token = pos_embed_temp[:,:1]
        pos_tokens = pos_embed_temp[:,1:]
        
        pos_tokens = pos_tokens.reshape(-1,original_size,original_size,self.emb_size).permute(0,3,1,2)

        pos_tokens = torch.nn.functional.interpolate(
            pos_tokens, size=(patches_along_height, patches_along_width), mode='bicubic', align_corners=False)
        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
        new_pos_embed = torch.cat((cls_token, pos_tokens), dim=1)

        return new_pos_embed.to(device)

'''Multi Headed Attention Implementation
  1. Extract the query, key and value vectors from input
  2. Add Pretrained weights to both qkv and the projection layers
  3. Compute self attention using q, k, v and concatenate the outputs'''

class MultiHeadAttention(nn.Module):
    def __init__(self,block, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):
        super().__init__()
        self.embedding = emb_size
        self.num_heads = num_heads

        '''We will use the queries, keys and values in a single vector'''
        self.qkv = nn.Linear(emb_size, emb_size * 3)
        self.att_drop = nn.Dropout(dropout)
        self.projection = nn.Linear(emb_size, emb_size)

        '''Add the Pretrained Query, Key, Value Weights'''
        weight_str = "blocks." + str(block) + ".attn.qkv.weight"
        bias_str = "blocks." + str(block) + ".attn.qkv.bias" 
        self.qkv.weight = nn.Parameter(weights[weight_str])
        self.qkv.bias = nn.Parameter(weights[bias_str])
               
        '''Add the Projection Weights'''
        weight_str = "blocks." + str(block) + ".attn.proj.weight"
        bias_str = "blocks." + str(block) + ".attn.proj.bias" 
        self.projection.weight = nn.Parameter(weights[weight_str])
        self.projection.bias = nn.Parameter(weights[bias_str])
      
    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:
        '''Extract the queries, keys and values from input
           The rearrangement is difficult to compute from torch.permute and torch.reshape, 
           so use einops library that provides easy rearrangement of tensors https://github.com/arogozhnikov/einops'''
        qkv = rearrange(self.qkv(x), "b n (h d qkv) -> (qkv) b h n d", h=self.num_heads, qkv=3)
        queries, keys, values = qkv[0], qkv[1], qkv[2]
        
        '''Use torch.einsum in place of matmul. Einstein Summation provides a much easier way to handle high dimenaional tensors'''
        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len
        
        '''Q*K is divided by scaling as described in formula of attention, and passed through a softmax layer'''
        scaling = self.embedding ** (1/2)
        attention = F.softmax(energy, dim=-1) / scaling
        attention = self.att_drop(attention)

        '''Matrix product of attention with Value vector, and rearrange to make output of the same dimension as input'''
        out = torch.einsum('bhal, bhlv -> bhav ', attention, values)
        batch, heads, patches, _=out.shape
        out=out.permute(0, 2, 1, 3).reshape(batch, patches, -1 )
        out = self.projection(out)
        return out

'''Make Residual connections through multiple transformer layers
   Add a Residual connection function'''

class ResidualAdd(nn.Module):
    def __init__(self, fn, block, norm):
        super().__init__()
        self.fn = fn

        '''Add Weights for the Layer'''
        weight_str = "blocks." + str(block) + ".norm"+ str(norm)+ ".weight"
        bias_str = "blocks." + str(block) + ".norm"+ str(norm)+ ".bias"
        self.fn[0].weights = nn.Parameter(weights[weight_str])
        self.fn[0].bias = nn.Parameter(weights[bias_str])
       
    def forward(self, x):
        res = x
        x = self.fn(x)
        x += res
        return x

'''Implement the FeedForward Network (MLP) Part of Transformer'''

class FeedForwardBlock(nn.Module):
    def __init__(self,block, emb_size: int, expansion: int = 4, drop_p: float = 0.):
        super().__init__()
        self.ffn = nn.Sequential(
                                    nn.Linear(emb_size, expansion * emb_size),
                                    nn.GELU(),
                                    nn.Dropout(drop_p),
                                    nn.Linear(expansion * emb_size, emb_size),
                                )
        '''Implement the First Fully Connected Layer'''
        weight_str = "blocks." + str(block) + ".mlp.fc1.weight"
        bias_str = "blocks." + str(block) + ".mlp.fc1.bias"
        self.ffn[0].weight = nn.Parameter(weights[weight_str])
        self.ffn[0].bias = nn.Parameter(weights[bias_str])

        '''Implement the Second Fully Connected Layer'''
        weight_str = "blocks." + str(block) + ".mlp.fc2.weight"
        bias_str = "blocks." + str(block) + ".mlp.fc2.bias"
        self.ffn[3].weight = nn.Parameter(weights[weight_str])
        self.ffn[3].bias = nn.Parameter(weights[bias_str])
     
    def forward(self,input):
        out = self.ffn(input)
        return out

'''Use The ResidualAdd Block we created to get residual Connections and implement a single transformer block'''

class TransformerEncoderBlock(nn.Sequential):
    def __init__(self,
                 block,
                 emb_size: int = 768,
                 drop_p: float = 0.,
                 forward_expansion: int = 4,
                 forward_drop_p: float = 0.,
                 ** kwargs):
        super().__init__(

            ResidualAdd(
                nn.Sequential(
                    nn.LayerNorm(emb_size),
                    MultiHeadAttention(block = block,
                                        emb_size = emb_size, 
                                        **kwargs),
                    nn.Dropout(drop_p)
                ),
                block=block, norm = 1
            ),
        
            ResidualAdd(
                nn.Sequential(
                    nn.LayerNorm(emb_size),
                    FeedForwardBlock(block = block,
                                    emb_size = emb_size, 
                                    expansion=forward_expansion, 
                                    drop_p=forward_drop_p),
                
                    nn.Dropout(drop_p)
                ),
                block= block, norm = 2 
            )
        )

'''Add a number of Transformer Encoder Blocks to our model'''

class TransformerEncoder(nn.Sequential):
    def __init__(self, depth: int = 12):
        super().__init__(*[TransformerEncoderBlock(block = i) for i in range(depth)])

'''Add Decoder as mentioned in the paper
   1. (Conv2d->BatchNorm->ReLU) -> (Conv2d->BatchNorm->ReLU)
   2. 1x1 Conv for feature extractor
   No Pretrained weights for this'''

class Decoder(nn.Module):
    def __init__(self,     
                img_size,
                patch_size,
                in_channels: int = 768,
                kernel_size: tuple = (4,4),
                intermediale_channels: int = 256,
                out_channels: int = 17,
                ):
    
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.deconvolution_layers = nn.Sequential(
                                nn.ConvTranspose2d(in_channels = in_channels,
                                                out_channels=intermediale_channels,
                                                kernel_size=kernel_size,
                                                stride = 2,
                                                padding = 1),
                                nn.BatchNorm2d(intermediale_channels),
                                nn.ReLU(inplace=True),

                                nn.ConvTranspose2d(in_channels = intermediale_channels,
                                                out_channels= intermediale_channels,
                                                kernel_size=kernel_size,
                                                stride=2,
                                                padding=1),
                                nn.BatchNorm2d(intermediale_channels),
                                nn.ReLU(inplace = True)
        )

        self.last_conv = nn.Conv2d(in_channels = intermediale_channels,
                                        out_channels=out_channels,
                                        kernel_size = 1,
                                        stride=1,
                                        padding=0)

    def forward(self,input):
        batch,_,_= input.shape

        input = input.permute(0,2,1).reshape(batch,-1,self.img_size[0]//self.patch_size,self.img_size[1]//self.patch_size)
        
        deconv_ = self.deconvolution_layers(input)
        
        out = self.last_conv(deconv_)
        
        return out

'''Initialize the Vision Transformer Class with the patch embedding and the Transformer Encoder layers'''

class ViT(nn.Sequential):
    def __init__(self,     
                in_channels: int = 3,
                patch_size: int = 16,
                emb_size: int = 768,
                img_size: tuple = (192, 256),
                depth: int = 12,                     
                kernel_size = (4,4),
                intermediale_channels: int = 256,
                out_channels: int = 17
                ):
        super().__init__(
            PatchEmbedding(in_channels, patch_size, emb_size, img_size),
            TransformerEncoder(depth, emb_size=emb_size)
            
        )

'''Initialize the ViT Pose Class with ViT Backbone and a Decoder, that we coded above'''

class ViTPose(nn.Module):
    def __init__(self,     
                in_channels: int = 3,
                patch_size: int = 16,
                emb_size: int = 768,
                img_size: tuple = (192, 256),
                depth: int = 12,                     #Depth of transformer layer
                kernel_size = (4,4),
                deconv_filter: int = 256,
                out_channels: int = 17
                ):
        super().__init__()


        self.model = ViT(in_channels,patch_size,emb_size,img_size,depth,kernel_size,deconv_filter,out_channels)
        self.decoder = Decoder(img_size,patch_size,emb_size, kernel_size, deconv_filter, out_channels)
    def forward(self,image):
        out_ = self.model(image)
        out = self.decoder(out_)
        return out